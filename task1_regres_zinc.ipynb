{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Regression on ZINC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive/\")\n",
    "    path = \"/content/drive/MyDrive/yanming_dissertation/gt_v2/code\"\n",
    "    os.chdir(path)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0\n",
    "    !pip install rdkit==2023.09.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, \"lib/\")\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule, compute_ncut\n",
    "import os, datetime\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# PyTorch version and GPU\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "  device= torch.device(\"cuda\")\n",
    "else:\n",
    "  device= torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "dataset/ZINC/\n",
      "Time: 1.6418 sec\n",
      "num train data : 2000\n",
      "atom_dict.idx2word : ['C', 'O', 'N', 'F', 'C H1', 'S', 'Cl', 'O -', 'N H1 +', 'Br', 'N H3 +', 'N H2 +', 'N +', 'N -', 'I', 'S -', 'P', 'N H1 -']\n",
      "atom_dict.word2idx : {'C': 0, 'O': 1, 'N': 2, 'F': 3, 'C H1': 4, 'S': 5, 'Cl': 6, 'O -': 7, 'N H1 +': 8, 'Br': 9, 'N H3 +': 10, 'N H2 +': 11, 'N +': 12, 'N -': 13, 'I': 14, 'S -': 15, 'P': 16, 'N H1 -': 17}\n",
      "bond_dict.idx2word : ['NONE', 'SINGLE', 'DOUBLE', 'TRIPLE']\n",
      "bond_dict.word2idx : {'NONE': 0, 'SINGLE': 1, 'DOUBLE': 2, 'TRIPLE': 3}\n",
      "18 4\n",
      "train[idx].atom_type : tensor([0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 2, 0, 5])\n",
      "train[idx].atom_type_pe : tensor([ 0,  1,  2,  3,  4,  5,  6,  0,  1,  7,  2,  8,  9, 10, 11, 12,  3, 13,\n",
      "        14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,  0,  4, 25,  0])\n",
      "train[idx].bond_type : tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 2, 0],\n",
      "        [0, 0, 0,  ..., 2, 0, 1],\n",
      "        [0, 0, 0,  ..., 0, 1, 0]])\n",
      "train[idx].bag_of_atoms : tensor([26,  1,  5,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "train[idx].smile:  CCCCCCC1=NN2C(=N)C(=CC3=C(C)N(C4=CC=C(C)C=C4C)C(C)=C3)C(=O)N=C2S1\n",
      "train[idx].logP_SA tensor([3.4121])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x22a8677e740>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "start = time.time()\n",
    "\n",
    "data_folder_pytorch = \"dataset/ZINC/\"\n",
    "print(data_folder_pytorch)\n",
    "\n",
    "with open(data_folder_pytorch+\"atom_dict.pkl\",\"rb\") as f:\n",
    "    atom_dict=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"bond_dict.pkl\",\"rb\") as f:\n",
    "    bond_dict=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"train.pkl\",\"rb\") as f:\n",
    "    train=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"val.pkl\",\"rb\") as f:\n",
    "    val=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"test.pkl\",\"rb\") as f:\n",
    "    test=pickle.load(f)\n",
    "\n",
    "print(f\"Time: {time.time() - start:.4f} sec\")\n",
    "\n",
    "print(\"num train data :\", len(train))\n",
    "print(\"atom_dict.idx2word :\", atom_dict.idx2word)\n",
    "print(\"atom_dict.word2idx :\", atom_dict.word2idx)\n",
    "print(\"bond_dict.idx2word :\", bond_dict.idx2word)\n",
    "print(\"bond_dict.word2idx :\", bond_dict.word2idx)\n",
    "\n",
    "num_atom_type = len(atom_dict.idx2word)\n",
    "num_bond_type = len(bond_dict.idx2word)\n",
    "print(num_atom_type, num_bond_type)\n",
    "\n",
    "idx = 0\n",
    "print(\"train[idx].atom_type :\", train[idx].atom_type)\n",
    "print(\"train[idx].atom_type_pe :\", train[idx].atom_type_pe)\n",
    "print(\"train[idx].bond_type :\", train[idx].bond_type)\n",
    "print(\"train[idx].bag_of_atoms :\", train[idx].bag_of_atoms)\n",
    "print(\"train[idx].smile: \", train[idx].smile)\n",
    "print(\"train[idx].logP_SA\", train[idx].logP_SA)\n",
    "mol = Chem.MolFromSmiles(train[idx].smile)\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max num atoms =  37\n",
      "\n",
      "Test\n",
      "number of molecule of size 12: \t 1\n",
      "number of molecule of size 13: \t 1\n",
      "number of molecule of size 14: \t 2\n",
      "number of molecule of size 15: \t 5\n",
      "number of molecule of size 16: \t 4\n",
      "number of molecule of size 17: \t 7\n",
      "number of molecule of size 18: \t 16\n",
      "number of molecule of size 19: \t 7\n",
      "number of molecule of size 20: \t 11\n",
      "number of molecule of size 21: \t 17\n",
      "number of molecule of size 22: \t 8\n",
      "number of molecule of size 23: \t 14\n",
      "number of molecule of size 24: \t 21\n",
      "number of molecule of size 25: \t 16\n",
      "number of molecule of size 26: \t 12\n",
      "number of molecule of size 27: \t 21\n",
      "number of molecule of size 28: \t 5\n",
      "number of molecule of size 29: \t 11\n",
      "number of molecule of size 30: \t 3\n",
      "number of molecule of size 31: \t 7\n",
      "number of molecule of size 32: \t 4\n",
      "number of molecule of size 33: \t 4\n",
      "number of molecule of size 34: \t 2\n",
      "number of molecule of size 36: \t 1\n"
     ]
    }
   ],
   "source": [
    "# Organize data into group of of molecules of fixed sized\n",
    "# Example: train[22] is a list containing all the molecules of size 22\n",
    "def group_molecules_per_size(dataset):\n",
    "    mydict = {}\n",
    "    for mol in dataset:\n",
    "        if len(mol) not in mydict:\n",
    "            mydict[len(mol)] = []\n",
    "        mydict[len(mol)].append(mol)\n",
    "    return mydict\n",
    "\n",
    "test_group  = group_molecules_per_size(test)\n",
    "val_group   = group_molecules_per_size(val)\n",
    "train_group = group_molecules_per_size(train)\n",
    "\n",
    "# The biggest molecule in the train set\n",
    "max_mol_sz= max(list( train_group.keys()))\n",
    "print(\"Max num atoms = \", max_mol_sz)\n",
    "\n",
    "# Print distribution w.r.t. molecule size\n",
    "def print_distribution(data):\n",
    "    for nb_atom in range(max_mol_sz+1):\n",
    "        try:\n",
    "            print(\"number of molecule of size {}: \\t {}\".format(nb_atom, len(data[nb_atom])))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print()\n",
    "# print(\"Train\"); print_distribution(train_group)\n",
    "# print(\"Val\"); print_distribution(val_group)\n",
    "print(\"Test\"); print_distribution(test_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Batches\n",
    "\n",
    "### Implement the molecule sampler class for batch sampling of molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeSampler:\n",
    "    def __init__(self, organized_dataset, bs, shuffle=True):\n",
    "        self.bs = bs\n",
    "        self.num_mol =  {sz: len(list_of_mol) for sz, list_of_mol in organized_dataset.items()}\n",
    "        self.counter = {sz: 0   for sz in organized_dataset}\n",
    "        if shuffle:\n",
    "            self.order = {sz: np.random.permutation(num)  for sz , num in self.num_mol.items()}\n",
    "        else:\n",
    "            self.order = {sz: np.arange(num)  for sz , num in self.num_mol.items()}\n",
    "\n",
    "    def compute_num_batches_remaining(self):\n",
    "        return {sz:  math.ceil(((self.num_mol[sz] - self.counter[sz])/self.bs))  for sz in self.num_mol}\n",
    "\n",
    "    def choose_molecule_size(self):\n",
    "        num_batches = self.compute_num_batches_remaining()\n",
    "        possible_sizes = np.array(list(num_batches.keys()))\n",
    "        prob = np.array(list(num_batches.values()))\n",
    "        prob = prob / prob.sum()\n",
    "        sz   = np.random.choice(possible_sizes, p=prob)\n",
    "        return sz\n",
    "\n",
    "    def is_empty(self):\n",
    "        num_batches= self.compute_num_batches_remaining()\n",
    "        return sum(num_batches.values()) == 0\n",
    "\n",
    "    def draw_batch_of_molecules(self, sz):\n",
    "        if (self.num_mol[sz] - self.counter[sz]) / self.bs >= 1.0:\n",
    "            bs = self.bs\n",
    "        else:\n",
    "            bs = self.num_mol[sz] - (self.num_mol[sz] // self.bs) * self.bs\n",
    "        indices = self.order[sz][self.counter[sz]:self.counter[sz] + bs]\n",
    "        self.counter[sz] += bs\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract one mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler.num_mol : {33: 22, 18: 89, 26: 174, 16: 51, 32: 29, 22: 159, 34: 13, 27: 135, 23: 175, 20: 145, 25: 169, 28: 85, 24: 183, 19: 107, 29: 69, 14: 24, 36: 4, 21: 152, 31: 44, 17: 61, 30: 37, 11: 4, 12: 5, 13: 17, 35: 10, 15: 30, 37: 3, 7: 1, 10: 2, 9: 1}\n",
      "num_batches_remaining : {33: 1, 18: 2, 26: 4, 16: 2, 32: 1, 22: 4, 34: 1, 27: 3, 23: 4, 20: 3, 25: 4, 28: 2, 24: 4, 19: 3, 29: 2, 14: 1, 36: 1, 21: 4, 31: 1, 17: 2, 30: 1, 11: 1, 12: 1, 13: 1, 35: 1, 15: 1, 37: 1, 7: 1, 10: 1, 9: 1}\n",
      "sz : 10\n",
      "indices : 2 [1 0]\n",
      "minibatch_node : torch.Size([2, 10])\n",
      "minibatch_pe : torch.Size([2, 10])\n",
      "minibatch_edge : torch.Size([2, 10, 10])\n",
      "minibatch_boa : torch.Size([2, 18])\n"
     ]
    }
   ],
   "source": [
    "# extract one mini-batch\n",
    "bs = 50\n",
    "sampler = MoleculeSampler(train_group, bs)\n",
    "print('sampler.num_mol :', sampler.num_mol)\n",
    "\n",
    "num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "print('num_batches_remaining :', num_batches_remaining)\n",
    "sz = sampler.choose_molecule_size()\n",
    "print('sz :', sz)\n",
    "indices = sampler.draw_batch_of_molecules(sz)\n",
    "print('indices :', len(indices), indices)\n",
    "minibatch_node = torch.stack([train_group[sz][i].atom_type for i in indices])\n",
    "print('minibatch_node :', minibatch_node.size())\n",
    "minibatch_pe  = torch.stack([train_group[sz][i].atom_type_pe for i in indices])\n",
    "print('minibatch_pe :', minibatch_pe.size())\n",
    "minibatch_edge = torch.stack([ train_group[sz][i].bond_type for i in indices])\n",
    "print('minibatch_edge :', minibatch_edge.size())\n",
    "minibatch_boa = torch.stack([train_group[sz][i].bag_of_atoms for i in indices])\n",
    "print('minibatch_boa :', minibatch_boa.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Tranformer Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d, num_heads, num_layers, drop :  128 8 4 0.0\n",
      "num_warmup : 80\n"
     ]
    }
   ],
   "source": [
    "# Global constants\n",
    "num_heads = 8; d = 16 * num_heads; num_layers = 4; drop = 0.0; bs = 50\n",
    "print(\"d, num_heads, num_layers, drop : \", d, num_heads, num_layers, drop)\n",
    "\n",
    "# Warmup\n",
    "num_mol_size = 20\n",
    "num_warmup = 2 * max(num_mol_size, len(train) // bs)\n",
    "print('num_warmup :', num_warmup)\n",
    "\n",
    "# Symmetric tensor function\n",
    "def sym_tensor(x):\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    triu = torch.triu(x, diagonal=1).transpose(3, 2)\n",
    "    mask = (triu.abs()>0).float()\n",
    "    x =  x * (1 - mask) + mask * triu\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Transformer Version 1 (GT-V1)\n",
    "\n",
    "#### Define the GT-V1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT Version 1\n",
    "class head_attention_v1(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d, d_head)\n",
    "        self.K = nn.Linear(d, d_head)\n",
    "        self.E = nn.Linear(d, d_head)\n",
    "        self.V = nn.Linear(d, d_head)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.drop_att = nn.Dropout(drop)\n",
    "        self.Ni = nn.Linear(d, d_head)\n",
    "        self.Nj = nn.Linear(d, d_head)\n",
    "    def forward(self, x, e):\n",
    "        Q = self.Q(x) # [bs, n, d_head]\n",
    "        K = self.K(x) # [bs, n, d_head]\n",
    "        V = self.V(x) # [bs, n, d_head]\n",
    "        Q = Q.unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        K = K.unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        E = self.E(e) # [bs, n, n, d_head]\n",
    "        Ni = self.Ni(x).unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        Nj = self.Nj(x).unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        e = Ni + Nj + E\n",
    "        Att = (Q * e * K).sum(dim=3) / self.sqrt_d # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=1) # [bs, n, n]\n",
    "        Att = self.drop_att(Att)\n",
    "        x = Att @ V  # [bs, n, d_head]\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the GT-V2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT Version 2\n",
    "class attention_node_to_edge(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q_node = nn.Linear(d, d_head)\n",
    "        self.K_edge = nn.Linear(d, d_head)\n",
    "        self.V_edge = nn.Linear(d, d_head)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.drop_att = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        Q_node = self.Q_node(x) # [bs, n, d_head]\n",
    "        K_edge = self.K_edge(e) # [bs, n, n, d_head]\n",
    "        V_edge = self.V_edge(e) # [bs, n, n, d_head]\n",
    "        Q_node = Q_node.unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        Att = (Q_node * K_edge).sum(dim=3) / self.sqrt_d # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=2)\n",
    "        Att = self.drop_att(Att)\n",
    "        Att = Att.unsqueeze(-1) # [bs, n, n, 1]\n",
    "        x = (Att * V_edge).sum(dim=2) # [bs, n, d_head]\n",
    "        return x, e # [bs, n, d_head]\n",
    "\n",
    "\n",
    "class attention_edge_to_node(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q_edge = nn.Linear(d, d_head)\n",
    "        self.K_node = nn.Linear(d, d_head)\n",
    "        self.V_node = nn.Linear(d, d_head)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.drop_att = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        Q_edge = self.Q_edge(e) # [bs, n, n, d_head]\n",
    "        K_node = self.K_node(x) # [bs, n, d_head]\n",
    "        V_node = self.V_node(x) # [bs, n, d_head]\n",
    "        K_i = K_node.unsqueeze(1).expand(-1, e.size(1), -1, -1) # [bs, n, n, d_head]\n",
    "        V_i = V_node.unsqueeze(1).expand(-1, e.size(1), -1, -1) # [bs, n, n, d_head]\n",
    "        K_j = K_node.unsqueeze(2).expand(-1, -1, e.size(1), -1) # [bs, n, n, d_head]\n",
    "        V_j = V_node.unsqueeze(2).expand(-1, -1, e.size(1), -1) # [bs, n, n, d_head]\n",
    "        Att_i = torch.exp((Q_edge * K_i).sum(dim=-1) / self.sqrt_d) # [bs, n, n]\n",
    "        Att_j = torch.exp((Q_edge * K_j).sum(dim=-1) / self.sqrt_d) # [bs, n, n]\n",
    "        Att_sum = Att_i + Att_j\n",
    "        Att_i = self.drop_att(Att_i / Att_sum)\n",
    "        Att_j = self.drop_att(Att_j / Att_sum)\n",
    "        e = Att_i.unsqueeze(-1) * V_i + Att_j.unsqueeze(-1) * V_j # [bs, n, n, d_head]\n",
    "        return x, e\n",
    "\n",
    "\n",
    "class attention_node_to_node(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d, d_head)  # For node queries\n",
    "        self.K = nn.Linear(d, d_head)  # For node keys\n",
    "        self.V = nn.Linear(d, d_head)  # For node values\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x, e):\n",
    "        Q = self.Q(x)  # [bs, n, d_head]\n",
    "        K = self.K(x)  # [bs, n, d_head]\n",
    "        V = self.V(x)  # [bs, n, d_head]\n",
    "        Q = Q.unsqueeze(2)  # [bs, n, 1, d_head]\n",
    "        K = K.unsqueeze(1)  # [bs, 1, n, d_head]\n",
    "        Att = (Q * K).sum(dim=-1) / self.sqrt_d  # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=-1)  # [bs, n, n]\n",
    "        Att = self.dropout(Att)\n",
    "        x = Att @ V  # [bs, n, d_head]\n",
    "        return x, e\n",
    "    \n",
    "\n",
    "class head_attention_v2(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.cross_att_node_to_edge = attention_node_to_edge(d, d_head)\n",
    "        self.cross_att_edge_to_node = attention_edge_to_node(d, d_head)\n",
    "        self.cross_att_node_to_node = attention_node_to_node(d, d_head)\n",
    "    def forward(self, x, e):\n",
    "        # 1) Cross-attention from nodes to edges\n",
    "        x_cross, _ = self.cross_att_node_to_edge(x, e)\n",
    "        # 2) Cross-attention from edges to nodes\n",
    "        _, e = self.cross_att_edge_to_node(x, e)\n",
    "        # 3) Self-attention on nodes\n",
    "        x_self, _ = self.cross_att_node_to_node(x, e)\n",
    "        x = (x_cross + x_self) / 2\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the GT network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads\n",
    "        self.heads = nn.ModuleList([head_attention_v2(d, d_head) for _ in range(num_heads)])\n",
    "        self.WOx = nn.Linear(d, d)\n",
    "        self.WOe = nn.Linear(d, d)\n",
    "        self.drop_x = nn.Dropout(drop)\n",
    "        self.drop_e = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        x_MHA = []\n",
    "        e_MHA = []\n",
    "        for head in self.heads:\n",
    "            x_HA, e_HA = head(x,e)\n",
    "            x_MHA.append(x_HA)\n",
    "            e_MHA.append(e_HA)\n",
    "        x = self.WOx(torch.cat(x_MHA, dim=2)) # [bs, n, d]\n",
    "        x = self.drop_x(x)\n",
    "        e = self.WOe(torch.cat(e_MHA, dim=3)) # [bs, n, n, d]\n",
    "        e = self.drop_e(e)\n",
    "        return x, e\n",
    "\n",
    "class BlockGT(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        self.LNx = nn.LayerNorm(d)\n",
    "        self.LNx2 = nn.LayerNorm(d)\n",
    "        self.MLPx = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.MHA = MHA(d, num_heads)\n",
    "        self.drop_x_mlp = nn.Dropout(drop)\n",
    "        self.LNe = nn.LayerNorm(d)\n",
    "        self.LNe2 = nn.LayerNorm(d)\n",
    "        self.MLPe = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.drop_x_mlp = nn.Dropout(drop)\n",
    "        self.drop_e_mlp = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        x = self.LNx(x)\n",
    "        e = self.LNe(e)\n",
    "        x_MHA, e_MHA = self.MHA(x, e) # [bs, n, d], [bs, n, n, d]\n",
    "        x = x + x_MHA # [bs, n, d]\n",
    "        x = x + self.MLPx(self.LNx2(x)) # [bs, n, d]\n",
    "        x = self.drop_x_mlp(x)\n",
    "        e = e + e_MHA\n",
    "        e = e + self.MLPe(self.LNe2(e)) # [bs, n, n, d]\n",
    "        e = self.drop_e_mlp(e)\n",
    "        return x, e\n",
    "\n",
    "class GT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.atom_emb = nn.Embedding(num_atom_type, d)\n",
    "        self.bond_emb = nn.Embedding(num_bond_type, d)\n",
    "        num_layers_encoder = 4\n",
    "        self.BlockGT_encoder_layers = nn.ModuleList( [BlockGT(d, num_heads) for _ in range(num_layers_encoder)] )\n",
    "        self.ln_x_final = nn.LayerNorm(d)\n",
    "        self.linear_x_final = nn.Linear(d, 1, bias=True)\n",
    "        self.drop_x_emb = nn.Dropout(drop)\n",
    "        self.drop_e_emb = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        x = self.atom_emb(x) # [bs, n, d]\n",
    "        e = self.bond_emb(e) # [bs, n, n, d]\n",
    "        e = sym_tensor(e) # [bs, n, n, d]\n",
    "        x = self.drop_x_emb(x)\n",
    "        e = self.drop_e_emb(e)\n",
    "        for gt_layer in self.BlockGT_encoder_layers:\n",
    "            x, e = gt_layer(x, e)  # [bs, n, d], [bs, n, n, d]\n",
    "            e = sym_tensor(e)\n",
    "        mol_token = x.mean(1) # [bs, d]\n",
    "        x = self.ln_x_final(mol_token)\n",
    "        x = self.linear_x_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate and test the GT-V1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1787521 (1.79 million)\n",
      "sampler.num_mol : {33: 22, 18: 89, 26: 174, 16: 51, 32: 29, 22: 159, 34: 13, 27: 135, 23: 175, 20: 145, 25: 169, 28: 85, 24: 183, 19: 107, 29: 69, 14: 24, 36: 4, 21: 152, 31: 44, 17: 61, 30: 37, 11: 4, 12: 5, 13: 17, 35: 10, 15: 30, 37: 3, 7: 1, 10: 2, 9: 1}\n",
      "num_batches_remaining : {33: 1, 18: 2, 26: 4, 16: 2, 32: 1, 22: 4, 34: 1, 27: 3, 23: 4, 20: 3, 25: 4, 28: 2, 24: 4, 19: 3, 29: 2, 14: 1, 36: 1, 21: 4, 31: 1, 17: 2, 30: 1, 11: 1, 12: 1, 13: 1, 35: 1, 15: 1, 37: 1, 7: 1, 10: 1, 9: 1}\n",
      "sz : 17\n",
      "indices : 50 [34 23 39 44 57 13 38 58 28 59 29 19 32 25 20 54 33 12 27 51 42  7  4 41\n",
      "  3 60 47 53 50 15 14  1 45 52  0  6 46 30 35 56  5 17 36 10  9 11 40 49\n",
      "  2 43]\n",
      "minibatch_node : torch.Size([50, 17])\n",
      "minibatch_edge : torch.Size([50, 17, 17])\n",
      "batch_target : torch.Size([50, 1])\n",
      "batch_x_pred torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "net = GT()\n",
    "net = net.to(device)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# Test the forward pass, backward pass and gradient update with a single batch\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True)\n",
    "\n",
    "bs = 50\n",
    "sampler = MoleculeSampler(train_group, bs)\n",
    "print('sampler.num_mol :',sampler.num_mol)\n",
    "num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "print('num_batches_remaining :',num_batches_remaining)\n",
    "sz = sampler.choose_molecule_size()\n",
    "print('sz :',sz)\n",
    "indices = sampler.draw_batch_of_molecules(sz)\n",
    "print('indices :',len(indices),indices)\n",
    "batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "print('minibatch_node :',minibatch_node.size())\n",
    "batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "print('minibatch_edge :',minibatch_edge.size())\n",
    "batch_target = torch.stack( [ train_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "print('batch_target :',batch_target.size())\n",
    "\n",
    "batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "print('batch_x_pred',batch_x_pred.size())\n",
    "\n",
    "loss = nn.L1Loss()(batch_x_pred, batch_target)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "del net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train And Evaluate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "torch.cuda.empty_cache()\n",
    "net = GT()\n",
    "net = net.to(device)\n",
    "\n",
    "# Optimizer\n",
    "init_lr = 0.0001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min((t+1)/num_warmup, 1.0) ) # warmup scheduler\n",
    "scheduler_tracker = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True) # tracker scheduler\n",
    "\n",
    "num_warmup_batch = 0\n",
    "\n",
    "# Number of mini-batches per epoch\n",
    "nb_epochs = 100\n",
    "\n",
    "lossMAE = nn.L1Loss()\n",
    "\n",
    "print(\"num batch(before scheduler_tracker), num epoch(before scheduler_tracker), num_warmup_batch(current): \", \\\n",
    "      num_warmup, num_warmup//(len(train)//bs), num_warmup_batch)\n",
    "\n",
    "total_loss = moving_loss = -1\n",
    "list_loss = []\n",
    "start=time.time()\n",
    "for epoch in range(nb_epochs):\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    num_data = 0\n",
    "    net.train()\n",
    "\n",
    "    bs = 50\n",
    "    sampler = MoleculeSampler(train_group, bs)\n",
    "    while(not sampler.is_empty()):\n",
    "        num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "        sz = sampler.choose_molecule_size()\n",
    "        indices = sampler.draw_batch_of_molecules(sz)\n",
    "        bs2 = len(indices)\n",
    "        batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "        batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "        batch_target = torch.stack( [ train_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "        batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "        loss = lossMAE(batch_x_pred, batch_target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if num_warmup_batch < num_warmup:\n",
    "            scheduler_warmup.step() # warmup scheduler\n",
    "        num_warmup_batch += 1\n",
    "\n",
    "        # Compute stats\n",
    "        running_loss += bs2 * loss.detach().item()\n",
    "        num_batches += 1\n",
    "        num_data += bs2\n",
    "\n",
    "    # Test set\n",
    "    bs = 50\n",
    "    sampler = MoleculeSampler(test_group, bs)\n",
    "    running_test_loss = 0\n",
    "    num_test_data = 0\n",
    "    with torch.no_grad():\n",
    "        while(not sampler.is_empty()):\n",
    "            num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "            sz = sampler.choose_molecule_size()\n",
    "            indices = sampler.draw_batch_of_molecules(sz)\n",
    "            bs2 = len(indices)\n",
    "            batch_x0 = minibatch_node = torch.stack( [ test_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "            batch_e0 = minibatch_edge = torch.stack( [ test_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "            batch_target = torch.stack( [ test_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "            batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "            running_test_loss += bs2 * lossMAE(batch_x_pred, batch_target).detach().item()\n",
    "            num_test_data += bs2\n",
    "\n",
    "    # Average stats and display\n",
    "    mean_train_loss = running_loss/num_data\n",
    "    mean_test_loss = running_test_loss/num_test_data\n",
    "    if num_warmup_batch >= num_warmup:\n",
    "        scheduler_tracker.step(mean_train_loss) # tracker scheduler defined w.r.t. loss value\n",
    "        num_warmup_batch += 1\n",
    "    elapsed = (time.time()-start)/60\n",
    "    if not epoch%1:\n",
    "        line = 'epoch= ' + str(epoch) + '\\t time= ' + str(elapsed)[:6] + ' min' + '\\t lr= ' + \\\n",
    "        '{:.7f}'.format(optimizer.param_groups[0]['lr']) + '\\t train_loss= ' + str(mean_train_loss)[:6] + \\\n",
    "        '\\t test_loss= ' + str(mean_test_loss)[:6]\n",
    "        print(line)\n",
    "\n",
    "    # Check lr value\n",
    "    if optimizer.param_groups[0]['lr'] < 10**-6:\n",
    "      print(\"\\n lr is equal to min lr -- training stopped\\n\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training conducted over 50 epochs on the small ZINC dataset, presenting the mean loss (and standard deviation) from the last 5 epochs.\n",
    "\n",
    "|      Network       | Train Loss on ZINC | Test Loss on ZINC |\n",
    "| :----------------: | :----------------: | :---------------: |\n",
    "|       GT-V1        |    0.594(0.005)    |   0.597(0.044)    |\n",
    "|  GT-V2 (Averaged)  |                    |                   |\n",
    "|   GT-V2 (Gated)    |                    |                   |\n",
    "| GT-V2 (Sequential) |                    |                   |\n",
    "|   GT-V2 (Mixed)    |                    |                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
