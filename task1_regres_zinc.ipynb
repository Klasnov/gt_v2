{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Regression on ZINC dataset\n",
    "\n",
    "##### Xavier Bresson, Shao Yanming\n",
    "\n",
    "This notebook is adapted from the National University of Singapore CS5284 Graph Machine Learning course, the original tutorial code can be found at [here](https://github.com/xbresson/CS5284_2024/tree/main).\n",
    "\n",
    "This notebook mainly focus on optimizing the Graph Transformer (GT) model, and testing the model by regression task on the ZINC dataset. The ZINC dataset used here is a subset of the original one, which only contains 2,000 training samples and 200 testing samples.\n",
    "\n",
    "> Some code snippets are generated by GPT-4o, and some text descriptions are written by GPT-4o and o1-mini(preview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive/\")\n",
    "    path = \"/content/drive/MyDrive/yanming_dissertation/gt_v2/code\"\n",
    "    os.chdir(path)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0\n",
    "    !pip install rdkit==2023.09.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, \"lib/\")\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule, compute_ncut\n",
    "import os, datetime\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch version and GPU\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "  device= torch.device(\"cuda\")\n",
    "else:\n",
    "  device= torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "start = time.time()\n",
    "\n",
    "data_folder_pytorch = \"dataset/ZINC/\"\n",
    "print(data_folder_pytorch)\n",
    "\n",
    "with open(data_folder_pytorch+\"atom_dict.pkl\",\"rb\") as f:\n",
    "    atom_dict=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"bond_dict.pkl\",\"rb\") as f:\n",
    "    bond_dict=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"train.pkl\",\"rb\") as f:\n",
    "    train=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"val.pkl\",\"rb\") as f:\n",
    "    val=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"test.pkl\",\"rb\") as f:\n",
    "    test=pickle.load(f)\n",
    "\n",
    "print(f\"Time: {time.time() - start:.4f} sec\")\n",
    "\n",
    "print(\"num train data :\", len(train))\n",
    "print(\"atom_dict.idx2word :\", atom_dict.idx2word)\n",
    "print(\"atom_dict.word2idx :\", atom_dict.word2idx)\n",
    "print(\"bond_dict.idx2word :\", bond_dict.idx2word)\n",
    "print(\"bond_dict.word2idx :\", bond_dict.word2idx)\n",
    "\n",
    "num_atom_type = len(atom_dict.idx2word)\n",
    "num_bond_type = len(bond_dict.idx2word)\n",
    "print(num_atom_type, num_bond_type)\n",
    "\n",
    "idx = 0\n",
    "print(\"train[idx].atom_type :\", train[idx].atom_type)\n",
    "print(\"train[idx].atom_type_pe :\", train[idx].atom_type_pe)\n",
    "print(\"train[idx].bond_type :\", train[idx].bond_type)\n",
    "print(\"train[idx].bag_of_atoms :\", train[idx].bag_of_atoms)\n",
    "print(\"train[idx].smile: \", train[idx].smile)\n",
    "print(\"train[idx].logP_SA\", train[idx].logP_SA)\n",
    "mol = Chem.MolFromSmiles(train[idx].smile)\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data into group of of molecules of fixed sized\n",
    "# Example: train[22] is a list containing all the molecules of size 22\n",
    "def group_molecules_per_size(dataset):\n",
    "    mydict = {}\n",
    "    for mol in dataset:\n",
    "        if len(mol) not in mydict:\n",
    "            mydict[len(mol)] = []\n",
    "        mydict[len(mol)].append(mol)\n",
    "    return mydict\n",
    "\n",
    "test_group  = group_molecules_per_size(test)\n",
    "val_group   = group_molecules_per_size(val)\n",
    "train_group = group_molecules_per_size(train)\n",
    "\n",
    "# The biggest molecule in the train set\n",
    "max_mol_sz= max(list( train_group.keys()))\n",
    "print(\"Max num atoms = \", max_mol_sz)\n",
    "\n",
    "# Print distribution w.r.t. molecule size\n",
    "def print_distribution(data):\n",
    "    for nb_atom in range(max_mol_sz+1):\n",
    "        try:\n",
    "            print(\"number of molecule of size {}: \\t {}\".format(nb_atom, len(data[nb_atom])))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print()\n",
    "# print(\"Train\"); print_distribution(train_group)\n",
    "# print(\"Val\"); print_distribution(val_group)\n",
    "print(\"Test\"); print_distribution(test_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Batches\n",
    "\n",
    "### Implement the molecule sampler class for batch sampling of molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeSampler:\n",
    "    def __init__(self, organized_dataset, bs, shuffle=True):\n",
    "        self.bs = bs\n",
    "        self.num_mol =  {sz: len(list_of_mol) for sz, list_of_mol in organized_dataset.items()}\n",
    "        self.counter = {sz: 0   for sz in organized_dataset}\n",
    "        if shuffle:\n",
    "            self.order = {sz: np.random.permutation(num)  for sz , num in self.num_mol.items()}\n",
    "        else:\n",
    "            self.order = {sz: np.arange(num)  for sz , num in self.num_mol.items()}\n",
    "\n",
    "    def compute_num_batches_remaining(self):\n",
    "        return {sz:  math.ceil(((self.num_mol[sz] - self.counter[sz])/self.bs))  for sz in self.num_mol}\n",
    "\n",
    "    def choose_molecule_size(self):\n",
    "        num_batches = self.compute_num_batches_remaining()\n",
    "        possible_sizes = np.array(list(num_batches.keys()))\n",
    "        prob = np.array(list(num_batches.values()))\n",
    "        prob = prob / prob.sum()\n",
    "        sz   = np.random.choice(possible_sizes, p=prob)\n",
    "        return sz\n",
    "\n",
    "    def is_empty(self):\n",
    "        num_batches= self.compute_num_batches_remaining()\n",
    "        return sum(num_batches.values()) == 0\n",
    "\n",
    "    def draw_batch_of_molecules(self, sz):\n",
    "        if (self.num_mol[sz] - self.counter[sz]) / self.bs >= 1.0:\n",
    "            bs = self.bs\n",
    "        else:\n",
    "            bs = self.num_mol[sz] - (self.num_mol[sz] // self.bs) * self.bs\n",
    "        indices = self.order[sz][self.counter[sz]:self.counter[sz] + bs]\n",
    "        self.counter[sz] += bs\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract one mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract one mini-batch\n",
    "bs = 50\n",
    "sampler = MoleculeSampler(train_group, bs)\n",
    "print('sampler.num_mol :', sampler.num_mol)\n",
    "\n",
    "num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "print('num_batches_remaining :', num_batches_remaining)\n",
    "sz = sampler.choose_molecule_size()\n",
    "print('sz :', sz)\n",
    "indices = sampler.draw_batch_of_molecules(sz)\n",
    "print('indices :', len(indices), indices)\n",
    "minibatch_node = torch.stack([train_group[sz][i].atom_type for i in indices])\n",
    "print('minibatch_node :', minibatch_node.size())\n",
    "minibatch_pe  = torch.stack([train_group[sz][i].atom_type_pe for i in indices])\n",
    "print('minibatch_pe :', minibatch_pe.size())\n",
    "minibatch_edge = torch.stack([ train_group[sz][i].bond_type for i in indices])\n",
    "print('minibatch_edge :', minibatch_edge.size())\n",
    "minibatch_boa = torch.stack([train_group[sz][i].bag_of_atoms for i in indices])\n",
    "print('minibatch_boa :', minibatch_boa.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Tranformer Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "num_heads = 8; d = 16 * num_heads; num_layers = 4; drop = 0.0; bs = 50\n",
    "print(\"d, num_heads, num_layers, drop : \", d, num_heads, num_layers, drop)\n",
    "\n",
    "# Warmup\n",
    "num_mol_size = 20\n",
    "num_warmup = 2 * max(num_mol_size, len(train) // bs)\n",
    "print('num_warmup :', num_warmup)\n",
    "\n",
    "# Symmetric tensor function\n",
    "def sym_tensor(x):\n",
    "    x = x.permute(0, 3, 1, 2)\n",
    "    triu = torch.triu(x, diagonal=1).transpose(3, 2)\n",
    "    mask = (triu.abs()>0).float()\n",
    "    x =  x * (1 - mask) + mask * triu\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GT-V1 attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT Version 1\n",
    "class head_attention_v1(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d, d_head)\n",
    "        self.K = nn.Linear(d, d_head)\n",
    "        self.E = nn.Linear(d, d_head)\n",
    "        self.V = nn.Linear(d, d_head)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.drop_att = nn.Dropout(drop)\n",
    "        self.Ni = nn.Linear(d, d_head)\n",
    "        self.Nj = nn.Linear(d, d_head)\n",
    "    def forward(self, x, e):\n",
    "        Q = self.Q(x) # [bs, n, d_head]\n",
    "        K = self.K(x) # [bs, n, d_head]\n",
    "        V = self.V(x) # [bs, n, d_head]\n",
    "        Q = Q.unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        K = K.unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        E = self.E(e) # [bs, n, n, d_head]\n",
    "        Ni = self.Ni(x).unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        Nj = self.Nj(x).unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        e = Ni + Nj + E\n",
    "        Att = (Q * e * K).sum(dim=3) / self.sqrt_d # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=1) # [bs, n, n]\n",
    "        Att = self.drop_att(Att)\n",
    "        x = Att @ V  # [bs, n, d_head]\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GT-V2 attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-attention and cross-attention modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT Version 2\n",
    "class attention_node_to_edge(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q_node = nn.Linear(d, d_head)\n",
    "        self.K_edge = nn.Linear(d, d_head)\n",
    "        self.V_edge = nn.Linear(d, d_head)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.drop_att = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        Q_node = self.Q_node(x) # [bs, n, d_head]\n",
    "        K_edge = self.K_edge(e) # [bs, n, n, d_head]\n",
    "        V_edge = self.V_edge(e) # [bs, n, n, d_head]\n",
    "        Q_node = Q_node.unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        Att = (Q_node * K_edge).sum(dim=3) / self.sqrt_d # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=2)\n",
    "        Att = self.drop_att(Att)\n",
    "        Att = Att.unsqueeze(-1) # [bs, n, n, 1]\n",
    "        x = (Att * V_edge).sum(dim=2) # [bs, n, d_head]\n",
    "        return x, e # [bs, n, d_head]\n",
    "\n",
    "\n",
    "class attention_edge_to_node(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q_edge = nn.Linear(d, d_head)\n",
    "        self.K_node = nn.Linear(d, d_head)\n",
    "        self.V_node = nn.Linear(d, d_head)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.drop_att = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        Q_edge = self.Q_edge(e) # [bs, n, n, d_head]\n",
    "        K_node = self.K_node(x) # [bs, n, d_head]\n",
    "        V_node = self.V_node(x) # [bs, n, d_head]\n",
    "        K_i = K_node.unsqueeze(1).expand(-1, e.size(1), -1, -1) # [bs, n, n, d_head]\n",
    "        V_i = V_node.unsqueeze(1).expand(-1, e.size(1), -1, -1) # [bs, n, n, d_head]\n",
    "        K_j = K_node.unsqueeze(2).expand(-1, -1, e.size(1), -1) # [bs, n, n, d_head]\n",
    "        V_j = V_node.unsqueeze(2).expand(-1, -1, e.size(1), -1) # [bs, n, n, d_head]\n",
    "        Att_i = torch.exp((Q_edge * K_i).sum(dim=-1) / self.sqrt_d) # [bs, n, n]\n",
    "        Att_j = torch.exp((Q_edge * K_j).sum(dim=-1) / self.sqrt_d) # [bs, n, n]\n",
    "        Att_sum = Att_i + Att_j\n",
    "        Att_i = self.drop_att(Att_i / Att_sum)\n",
    "        Att_j = self.drop_att(Att_j / Att_sum)\n",
    "        e = Att_i.unsqueeze(-1) * V_i + Att_j.unsqueeze(-1) * V_j # [bs, n, n, d_head]\n",
    "        return x, e\n",
    "\n",
    "\n",
    "class attention_node_to_node(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d, d_head)  # For node queries\n",
    "        self.K = nn.Linear(d, d_head)  # For node keys\n",
    "        self.V = nn.Linear(d, d_head)  # For node values\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x, e):\n",
    "        Q = self.Q(x)  # [bs, n, d_head]\n",
    "        K = self.K(x)  # [bs, n, d_head]\n",
    "        V = self.V(x)  # [bs, n, d_head]\n",
    "        Q = Q.unsqueeze(2)  # [bs, n, 1, d_head]\n",
    "        K = K.unsqueeze(1)  # [bs, 1, n, d_head]\n",
    "        Att = (Q * K).sum(dim=-1) / self.sqrt_d  # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=-1)  # [bs, n, n]\n",
    "        Att = self.dropout(Att)\n",
    "        x = Att @ V  # [bs, n, d_head]\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different types of attention heads in GT-V2\n",
    "\n",
    "1. **Weighted integration**: define a fixed weight $\\alpha$ to integrate the self-attention and cross-attention results. The final output is computed as\n",
    "   $$\n",
    "   h_k = \\alpha \\cdot \\text{CrossAttention}(h^{\\ell}) + (1 - \\alpha) \\cdot \\text{SelfAttention}(h^{\\ell}).\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class head_attention_v2_weighted(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.alpha = 0.5\n",
    "        self.cross_att_node_to_edge = attention_node_to_edge(d, d_head)\n",
    "        self.cross_att_edge_to_node = attention_edge_to_node(d, d_head)\n",
    "        self.cross_att_node_to_node = attention_node_to_node(d, d_head)\n",
    "    def forward(self, x, e):\n",
    "        # 1) Cross-attention from nodes to edges\n",
    "        x_cross, _ = self.cross_att_node_to_edge(x, e)\n",
    "        # 2) Cross-attention from edges to nodes\n",
    "        _, e = self.cross_att_edge_to_node(x, e)\n",
    "        # 3) Self-attention on nodes\n",
    "        x_self, _ = self.cross_att_node_to_node(x, e)\n",
    "        x = self.alpha * x_cross + (1 - self.alpha) * x_self\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Gated integration**: instead of using a fixed weight for the combination, the model learns a dynamic gating mechanism to adjust the contributions of cross-attention and self-attention as\n",
    "   $$\n",
    "   g = \\sigma(W_g \\cdot \\text{Concat} [\\text{CrossAttention}(h^{\\ell}), \\text{SelfAttention}(h^{\\ell})] + b_g)  \\\\\n",
    "   h_k = g \\odot \\text{CrossAttention}(h^{\\ell}) + (1 - g) \\odot \\text{SelfAttention}(h^{\\ell}),\n",
    "   $$\n",
    "   where $\\sigma$ is the sigmoid function, and $W_g$ and $b_g$ are learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class head_attention_v2_gated(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.cross_att_node_to_edge = attention_node_to_edge(d, d_head)\n",
    "        self.cross_att_edge_to_node = attention_edge_to_node(d, d_head)\n",
    "        self.cross_att_node_to_node = attention_node_to_node(d, d_head)\n",
    "        self.gated_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * d_head, d_head),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x, e):\n",
    "        # 1) Cross-attention from nodes to edges\n",
    "        x_cross, _ = self.cross_att_node_to_edge(x, e)\n",
    "        # 2) Cross-attention from edges to nodes\n",
    "        _, e = self.cross_att_edge_to_node(x, e)\n",
    "        # 3) Self-attention on nodes\n",
    "        x_self, _ = self.cross_att_node_to_node(x, e)\n",
    "        # 4) Gated MLP\n",
    "        g = self.gated_mlp(torch.cat([x_cross, x_self], dim=-1))\n",
    "        x = g * x_cross + (1 - g) * x_self\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Mixed integration**: the model learns a linear combination of the cross-attention and self-attention as\n",
    "   $$\n",
    "   h_k = W_m \\cdot \\text{Concat} [\\text{CrossAttention}(h^{\\ell}), \\text{SelfAttention}(h^{\\ell})] + b_m,\n",
    "   $$\n",
    "   where $W_m$ and $b_m$ are learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class head_attention_v2_mixed(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.cross_att_node_to_edge = attention_node_to_edge(d, d_head)\n",
    "        self.cross_att_edge_to_node = attention_edge_to_node(d, d_head)\n",
    "        self.cross_att_node_to_node = attention_node_to_node(d, d_head)\n",
    "        self.mix_ll = nn.Linear(2 * d_head, d_head, bias=True)\n",
    "    def forward(self, x, e):\n",
    "        # 1) Cross-attention from nodes to edges\n",
    "        x_cross, _ = self.cross_att_node_to_edge(x, e)\n",
    "        # 2) Cross-attention from edges to nodes\n",
    "        _, e = self.cross_att_edge_to_node(x, e)\n",
    "        # 3) Self-attention on nodes\n",
    "        x_self, _ = self.cross_att_node_to_node(x, e)\n",
    "        # 4) Mixing\n",
    "        x = self.mix_ll(torch.cat([x_cross, x_self], dim=-1))\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GT network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads\n",
    "\n",
    "        # GT-V1\n",
    "        self.heads = nn.ModuleList([head_attention_v1(d, d_head) for _ in range(num_heads)])\n",
    "\n",
    "        # GT-V2 (weighted)\n",
    "        # self.heads = nn.ModuleList([head_attention_v2_weighted(d, d_head) for _ in range(num_heads)])\n",
    "\n",
    "        # GT-V2 (gated)\n",
    "        # self.heads = nn.ModuleList([head_attention_v2_gated(d, d_head) for _ in range(num_heads)])\n",
    "        \n",
    "        # GT-V2 (mixed)\n",
    "        # self.heads = nn.ModuleList([head_attention_v2_mixed(d, d_head) for _ in range(num_heads)])\n",
    "\n",
    "        self.WOx = nn.Linear(d, d)\n",
    "        self.WOe = nn.Linear(d, d)\n",
    "        self.drop_x = nn.Dropout(drop)\n",
    "        self.drop_e = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        x_MHA = []\n",
    "        e_MHA = []\n",
    "        for head in self.heads:\n",
    "            x_HA, e_HA = head(x,e)\n",
    "            x_MHA.append(x_HA)\n",
    "            e_MHA.append(e_HA)\n",
    "        x = self.WOx(torch.cat(x_MHA, dim=2)) # [bs, n, d]\n",
    "        x = self.drop_x(x)\n",
    "        e = self.WOe(torch.cat(e_MHA, dim=3)) # [bs, n, n, d]\n",
    "        e = self.drop_e(e)\n",
    "        return x, e\n",
    "\n",
    "class BlockGT(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        self.LNx = nn.LayerNorm(d)\n",
    "        self.LNx2 = nn.LayerNorm(d)\n",
    "        self.MLPx = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.MHA = MHA(d, num_heads)\n",
    "        self.drop_x_mlp = nn.Dropout(drop)\n",
    "        self.LNe = nn.LayerNorm(d)\n",
    "        self.LNe2 = nn.LayerNorm(d)\n",
    "        self.MLPe = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "        self.drop_x_mlp = nn.Dropout(drop)\n",
    "        self.drop_e_mlp = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        x = self.LNx(x)\n",
    "        e = self.LNe(e)\n",
    "        x_MHA, e_MHA = self.MHA(x, e) # [bs, n, d], [bs, n, n, d]\n",
    "        x = x + x_MHA # [bs, n, d]\n",
    "        x = x + self.MLPx(self.LNx2(x)) # [bs, n, d]\n",
    "        x = self.drop_x_mlp(x)\n",
    "        e = e + e_MHA\n",
    "        e = e + self.MLPe(self.LNe2(e)) # [bs, n, n, d]\n",
    "        e = self.drop_e_mlp(e)\n",
    "        return x, e\n",
    "\n",
    "class GT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.atom_emb = nn.Embedding(num_atom_type, d)\n",
    "        self.bond_emb = nn.Embedding(num_bond_type, d)\n",
    "        num_layers_encoder = 4\n",
    "        self.BlockGT_encoder_layers = nn.ModuleList( [BlockGT(d, num_heads) for _ in range(num_layers_encoder)] )\n",
    "        self.ln_x_final = nn.LayerNorm(d)\n",
    "        self.linear_x_final = nn.Linear(d, 1, bias=True)\n",
    "        self.drop_x_emb = nn.Dropout(drop)\n",
    "        self.drop_e_emb = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "        x = self.atom_emb(x) # [bs, n, d]\n",
    "        e = self.bond_emb(e) # [bs, n, n, d]\n",
    "        e = sym_tensor(e) # [bs, n, n, d]\n",
    "        x = self.drop_x_emb(x)\n",
    "        e = self.drop_e_emb(e)\n",
    "        for gt_layer in self.BlockGT_encoder_layers:\n",
    "            x, e = gt_layer(x, e)  # [bs, n, d], [bs, n, n, d]\n",
    "            e = sym_tensor(e)\n",
    "        mol_token = x.mean(1) # [bs, d]\n",
    "        x = self.ln_x_final(mol_token)\n",
    "        x = self.linear_x_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GT()\n",
    "net = net.to(device)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# Test the forward pass, backward pass and gradient update with a single batch\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True)\n",
    "\n",
    "bs = 50\n",
    "sampler = MoleculeSampler(train_group, bs)\n",
    "print('sampler.num_mol :',sampler.num_mol)\n",
    "num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "print('num_batches_remaining :',num_batches_remaining)\n",
    "sz = sampler.choose_molecule_size()\n",
    "print('sz :',sz)\n",
    "indices = sampler.draw_batch_of_molecules(sz)\n",
    "print('indices :',len(indices),indices)\n",
    "batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "print('minibatch_node :',minibatch_node.size())\n",
    "batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "print('minibatch_edge :',minibatch_edge.size())\n",
    "batch_target = torch.stack( [ train_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "print('batch_target :',batch_target.size())\n",
    "\n",
    "batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "print('batch_x_pred',batch_x_pred.size())\n",
    "\n",
    "loss = nn.L1Loss()(batch_x_pred, batch_target)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "del net\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train And Evaluate The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Training loop\n",
    "net = GT()\n",
    "net = net.to(device)\n",
    "\n",
    "# Optimizer\n",
    "init_lr = 0.0001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min((t+1)/num_warmup, 1.0) ) # warmup scheduler\n",
    "scheduler_tracker = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True) # tracker scheduler\n",
    "\n",
    "num_warmup_batch = 0\n",
    "\n",
    "# Number of mini-batches per epoch\n",
    "nb_epochs = 50\n",
    "\n",
    "lossMAE = nn.L1Loss()\n",
    "\n",
    "print(\"num batch(before scheduler_tracker), num epoch(before scheduler_tracker), num_warmup_batch(current): \", \\\n",
    "      num_warmup, num_warmup//(len(train)//bs), num_warmup_batch)\n",
    "\n",
    "total_loss = moving_loss = -1\n",
    "list_loss = []\n",
    "start=time.time()\n",
    "for epoch in range(nb_epochs):\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    num_data = 0\n",
    "    net.train()\n",
    "\n",
    "    bs = 50\n",
    "    sampler = MoleculeSampler(train_group, bs)\n",
    "    while(not sampler.is_empty()):\n",
    "        num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "        sz = sampler.choose_molecule_size()\n",
    "        indices = sampler.draw_batch_of_molecules(sz)\n",
    "        bs2 = len(indices)\n",
    "        batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "        batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "        batch_target = torch.stack( [ train_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "        batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "        loss = lossMAE(batch_x_pred, batch_target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if num_warmup_batch < num_warmup:\n",
    "            scheduler_warmup.step() # warmup scheduler\n",
    "        num_warmup_batch += 1\n",
    "\n",
    "        # Compute stats\n",
    "        running_loss += bs2 * loss.detach().item()\n",
    "        num_batches += 1\n",
    "        num_data += bs2\n",
    "\n",
    "    # Test set\n",
    "    bs = 50\n",
    "    sampler = MoleculeSampler(test_group, bs)\n",
    "    running_test_loss = 0\n",
    "    num_test_data = 0\n",
    "    with torch.no_grad():\n",
    "        while(not sampler.is_empty()):\n",
    "            num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "            sz = sampler.choose_molecule_size()\n",
    "            indices = sampler.draw_batch_of_molecules(sz)\n",
    "            bs2 = len(indices)\n",
    "            batch_x0 = minibatch_node = torch.stack( [ test_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "            batch_e0 = minibatch_edge = torch.stack( [ test_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "            batch_target = torch.stack( [ test_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "            batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "            running_test_loss += bs2 * lossMAE(batch_x_pred, batch_target).detach().item()\n",
    "            num_test_data += bs2\n",
    "\n",
    "    # Average stats and display\n",
    "    mean_train_loss = running_loss/num_data\n",
    "    mean_test_loss = running_test_loss/num_test_data\n",
    "    if num_warmup_batch >= num_warmup:\n",
    "        scheduler_tracker.step(mean_train_loss) # tracker scheduler defined w.r.t. loss value\n",
    "        num_warmup_batch += 1\n",
    "    elapsed = (time.time()-start)/60\n",
    "    if not epoch%1:\n",
    "        line = 'epoch= ' + str(epoch) + '\\t time= ' + str(elapsed)[:6] + ' min' + '\\t lr= ' + \\\n",
    "        '{:.7f}'.format(optimizer.param_groups[0]['lr']) + '\\t train_loss= ' + str(mean_train_loss)[:6] + \\\n",
    "        '\\t test_loss= ' + str(mean_test_loss)[:6]\n",
    "        print(line)\n",
    "\n",
    "    # Check lr value\n",
    "    if optimizer.param_groups[0]['lr'] < 10**-6:\n",
    "      print(\"\\n lr is equal to min lr -- training stopped\\n\")\n",
    "      break\n",
    "\n",
    "del net\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training was conducted over 50 epochs on a small subset of the ZINC dataset (2,000 training samples and 200 testing samples) with a fixed random seed. \n",
    "\n",
    "The results presented are the mean loss (and standard deviation) from the last 10 epochs.\n",
    "\n",
    "\n",
    "|             Network             | Train Loss on ZINC | Test Loss on ZINC  | Time (min) |\n",
    "| :-----------------------------: | :----------------: | :----------------: | :--------: |\n",
    "|              GT-V1              |   0.6090(0.0067)   |   0.5624(0.0416)   |   4.7717   |\n",
    "| GT-V2 (Weighted, $\\alpha$=0.25) |   0.5995(0.0072)   |   0.5218(0.0312)   |   8.0890   |\n",
    "| GT-V2 (Weighted, $\\alpha$=0.5)  |   0.5944(0.0099)   |   0.5308(0.0370)   |   7.8275   |\n",
    "| GT-V2 (Weighted, $\\alpha$=0.75) |   0.5923(0.0084)   |   0.5247(0.0321)   |   7.9889   |\n",
    "|          GT-V2 (Gated)          |   0.5939(0.0078)   |   0.5232(0.0285)   |   8.7069   |\n",
    "|          GT-V2 (Mixed)          | **0.5847**(0.0088) | **0.5068**(0.0176) |   8.4193   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Dwivedi, Vijay Prakash, and Xavier Bresson. \"A generalization of transformer networks to graphs.\" *arXiv preprint arXiv:2012.09699* (2020).](https://arxiv.org/abs/2012.09699)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
